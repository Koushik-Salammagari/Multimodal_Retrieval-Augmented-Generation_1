# -*- coding: utf-8 -*-
"""RAG_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mfX5ceEh03MUK_YMp-ED6EeE7Bk_fP6u
"""

# installing packages

# Commented out IPython magic to ensure Python compatibility.

# %pip install unstructured
# %pip install python-magic
# %pip install python-pptx

!pip install python-pptx PyMuPDF

!pip install pypdf2
!pip install python-pptx
!pip install sentence-transformers
!pip install transformers
!pip install langchain

# vector store
!pip install faiss-cpu

# extracting content from the ppts

from pptx import Presentation
import os

def extract_content_from_ppt(file_path, output_dir="ppt_images"):
    presentation = Presentation(file_path)
    content = {
        "text": [],
        "tables": [],
        "charts": [],
        "images": [],
    }

    # Ensure output directory exists for images
    os.makedirs(output_dir, exist_ok=True)

    # Iterate through slides
    for slide_idx, slide in enumerate(presentation.slides):
        slide_text = []

        for shape_idx, shape in enumerate(slide.shapes):
            # Text Frames
            if shape.has_text_frame:
                slide_text.append(shape.text_frame.text)

            # Tables
            if shape.has_table:
                table_data = []
                table = shape.table
                for row in table.rows:
                    row_data = [cell.text for cell in row.cells]
                    table_data.append(row_data)
                content["tables"].append({"slide": slide_idx, "table": table_data})

            # Images (Picture Shapes)
            if shape.shape_type == 13:  # 13 corresponds to 'Picture' shape
                image = shape.image
                image_bytes = image.blob
                image_path = os.path.join(output_dir, f"slide{slide_idx}_shape{shape_idx}.png")
                with open(image_path, "wb") as img_file:
                    img_file.write(image_bytes)
                content["images"].append({"slide": slide_idx, "path": image_path})

        # Append slide text
        content["text"].append({"slide": slide_idx, "text": " ".join(slide_text)})

    return content

# diplaying the structure of data

def display_content(content):
    print("Text Content:")
    for text_item in content["text"]:
        print(f"  Slide {text_item['slide']}: {text_item['text']}")

    print("\nTables:")
    for table_item in content["tables"]:
        print(f"  Slide {table_item['slide']}:")
        for row in table_item["table"]:
            print("    " + "\t".join(row))

    print("\nCharts:")
    if not content["charts"]:
        print("  No charts found.")

    print("\nImages:")
    for image_item in content["images"]:
        print(f"  Slide {image_item['slide']}: {image_item['path']}")

# Specify the path to your PPTX file
ppt_file_path = "Infectious Disease_ Meningitis by Slidesgo.pptx"  # Replace with the path to your .pptx file

# Call the function to extract content
parsed_content = extract_content_from_ppt(ppt_file_path)

# Display the extracted content
display_content(parsed_content)

# appended text in the previous code block

all_text = " ".join([item["text"] for item in parsed_content["text"]])
print("Combined Text for Analysis:\n", all_text)

# saving the tables as csv files

import csv

def save_tables_as_csv(content, output_dir="tables"):
    os.makedirs(output_dir, exist_ok=True)
    for idx, table_item in enumerate(content["tables"]):
        csv_file_path = os.path.join(output_dir, f"table_slide{table_item['slide']}.csv")
        with open(csv_file_path, "w", newline="") as csv_file:
            writer = csv.writer(csv_file)
            writer.writerows(table_item["table"])
        print(f"Table saved: {csv_file_path}")

# Save tables
save_tables_as_csv(parsed_content)

# now with images

from PIL import Image

def display_saved_images(content):
    for image_item in content["images"]:
        image_path = image_item["path"]
        print(f"Displaying Image: {image_path}")
        image = Image.open(image_path)
        image.show()  # This will open the image in your default viewer

# Display images
display_saved_images(parsed_content)

def consolidate_content(parsed_content):
    """
    Consolidate slide text and table content into a list of chunks.
    """
    combined_chunks = []

    # Add slide text
    for text_item in parsed_content["text"]:
        slide_text = text_item["text"].strip()
        if slide_text:
            combined_chunks.append(slide_text)

    # Add tables as plain text
    for table_item in parsed_content["tables"]:
        table_as_text = "\n".join([" | ".join(row) for row in table_item["table"]])
        combined_chunks.append(f"Table (Slide {table_item['slide']}):\n{table_as_text}")

    return combined_chunks

# Consolidate content after parsing
combined_content = consolidate_content(parsed_content)

# Check the consolidated content
for idx, chunk in enumerate(combined_content):
    print(f"Chunk {idx + 1}:\n{chunk}\n")

def chunk_content(combined_content, max_chunk_size=500):
    """
    Chunk consolidated content into smaller pieces for embeddings.
    """
    chunks = []
    for content in combined_content:
        words = content.split()
        chunks.extend([
            " ".join(words[i: i + max_chunk_size])
            for i in range(0, len(words), max_chunk_size)
        ])
    return chunks

# Chunk the content into smaller parts
chunked_content = chunk_content(combined_content, max_chunk_size=500)

# Display chunked content
for idx, chunk in enumerate(chunked_content):
    print(f"Chunk {idx + 1}:\n{chunk}\n")

## generating the embeddings
from sentence_transformers import SentenceTransformer

# Load a pre-trained embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and free model

def generate_embeddings(chunks):
    """
    Generate embeddings for each chunk of text.
    """
    return embedding_model.encode(chunks)

# Generate embeddings for the chunked content
embeddings = generate_embeddings(chunked_content)
print(f"Generated {len(embeddings)} embeddings.")

# installed FAISS previously

import faiss
import numpy as np

# Create a FAISS index
dimension = embeddings[0].shape[0]  # Dimension of embedding vectors
faiss_index = faiss.IndexFlatL2(dimension)  # L2 similarity metric

# Add embeddings to the index
faiss_index.add(np.array(embeddings))

# Map chunks to their embedding indices
chunk_mapping = {i: chunk for i, chunk in enumerate(chunked_content)}

print(f"FAISS index created with {faiss_index.ntotal} vectors.")

def query_embeddings(query, top_k=3):
    """
    Find the most relevant chunks for a given query using FAISS.
    """
    query_embedding = embedding_model.encode([query])[0]  # Generate query embedding
    distances, indices = faiss_index.search(np.array([query_embedding]), top_k)
    return [chunk_mapping[idx] for idx in indices[0]]  # Return the top-k relevant chunks

# Test querying
query = "Summarize the introduction slide."
relevant_chunks = query_embeddings(query, top_k=5)

print("Relevant Chunks for Query:")
for chunk in relevant_chunks:
    print(chunk)

# Now with llm

# from groq import Groq
# from groq import Groq

# # Initialize Groq client with the API key directly
# api_key = "llama-3.1-70b-versatile"
# client = Groq(api_key=api_key)


# # Function to initialize FAISS index and load embeddings into LangChain
# def initialize_faiss_index_and_chain(chunked_content):
#     """
#     Initialize FAISS index with LangChain and return a retrieval mechanism.
#     """
#     # Generate embeddings using LangChain
#     embedding_model = OpenAIEmbeddings()
#     vector_store = FAISS.from_texts(chunked_content, embedding_model)

#     # Create a retriever from the vector store
#     retriever = vector_store.as_retriever()

#     return retriever  # Only return retriever for use with Groq LLM

# # Function to query the Groq LLM with the retriever
# def query_with_groq(retriever, query, model="llama3-8b-8192", temperature=1, max_tokens=1024):
#     """
#     Use Groq's LLM to answer the query based on retrieved documents.
#     """
#     # Retrieve relevant documents
#     relevant_docs = retriever.get_relevant_documents(query)
#     context = "\n\n".join([doc.page_content for doc in relevant_docs])

#     # Initialize Groq client
#     client = Groq()

#     # Formulate messages for the chat completion
#     messages = [
#         {"role": "system", "content": "You are an assistant that answers questions based on the provided context."},
#         {"role": "user", "content": f"Context:\n{context}\n\nQuery: {query}"}
#     ]

#     # Call the Groq client
#     completion = client.chat.completions.create(
#         model=model,
#         messages=messages,
#         temperature=temperature,
#         max_tokens=max_tokens,
#         top_p=1,
#         stream=True,
#         stop=None,
#     )

#     # Collect the response
#     response = ""
#     for chunk in completion:
#         response += chunk.choices[0].delta.content or ""

#     return response

# # Consolidate content into chunks for processing
# combined_content = consolidate_content(parsed_content)
# chunked_content = chunk_content(combined_content, max_chunk_size=500)

# # Initialize FAISS index and retriever
# retriever = initialize_faiss_index_and_chain(chunked_content)

# # Test the system with a sample query using Groq LLM
# query = "Summarize the introduction slide and extract key points."
# response = query_with_groq(retriever, query)
# print("Response:", response)

!pip install langchain-groq

!pip install -qU langchain-community faiss-cpu

from groq import Groq
import os
from sentence_transformers import SentenceTransformer
from langchain.vectorstores import FAISS
from langchain.embeddings import SentenceTransformerEmbeddings

# Initialize Groq client
api_key = os.getenv("GROQ_API_KEY")
client = Groq(api_key='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')

def initialize_faiss_index_and_chain(chunked_content, model_name='all-MiniLM-L6-v2'):
    """
    Initialize FAISS index with SentenceTransformer embeddings.

    Args:
    - chunked_content (list): List of text chunks to embed
    - model_name (str): Name of the SentenceTransformer model to use

    Returns:
    - Retriever object for document retrieval
    """
    try:
        # Load the embedding model
        embedding_model = SentenceTransformer(model_name)

        # Create embedding function for LangChain
        embedding_function = SentenceTransformerEmbeddings(
            model_name=model_name
        )

        # Generate embeddings manually to have more control
        embeddings = embedding_model.encode(chunked_content)

        # Create FAISS vector store
        vector_store = FAISS.from_texts(
            texts=chunked_content,
            embedding=embedding_function
        )

        # Create and return retriever
        return vector_store.as_retriever(
            search_type="similarity",  # Can also use "mmr" for maximal marginal relevance
            search_kwargs={"k": 3}  # Return top 3 most similar chunks
        )

    except Exception as e:
        print(f"Error in initializing FAISS index: {e}")
        return None

def query_with_groq(retriever, query, model="llama3-8b-8192", temperature=0.7, max_tokens=1024):
    """
    Query Groq LLM with context from retrieved documents.

    Args:
    - retriever: Document retrieval mechanism
    - query (str): User's query
    - model (str): Groq LLM model to use
    - temperature (float): Sampling temperature
    - max_tokens (int): Maximum token generation

    Returns:
    - LLM's response
    """
    try:
        # Retrieve relevant documents
        relevant_docs = retriever.get_relevant_documents(query)

        # Combine retrieved document contents
        context = "\n\n".join([doc.page_content for doc in relevant_docs])

        # Prepare messages for LLM
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant that answers questions based on the provided context. If the context doesn't contain enough information, acknowledge that."
            },
            {
                "role": "user",
                "content": f"Context:\n{context}\n\nQuery: {query}"
            }
        ]

        # Call Groq LLM
        completion = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=1,
            stream=True,
            stop=None,
        )

        # Collect response
        response = ""
        for chunk in completion:
            if chunk.choices[0].delta.content:
                response += chunk.choices[0].delta.content

        return response

    except Exception as e:
        print(f"Error in querying Groq LLM: {e}")
        return "An error occurred while processing your query."

# Alternative Embedding Models
EMBEDDING_MODELS = {
    'default': 'all-MiniLM-L6-v2',
    'multilingual': 'paraphrase-multilingual-MiniLM-L12-v2',
    'medical': 'michiyasunaga/BioLinkBERT-base',
    'scientific': 'BAAI/bge-large-en-v1.5'
}

def choose_embedding_model(domain=None):
    """
    Select an appropriate embedding model based on domain.

    Args:
    - domain (str): Optional domain specification

    Returns:
    - str: Model name
    """
    if domain:
        domain_lower = domain.lower()
        if 'medical' in domain_lower:
            return EMBEDDING_MODELS['medical']
        elif 'science' in domain_lower:
            return EMBEDDING_MODELS['scientific']
        elif 'multi' in domain_lower:
            return EMBEDDING_MODELS['multilingual']

    return EMBEDDING_MODELS['default']

# Main Execution
def main():
    # Consolidate and chunk content
    combined_content = consolidate_content(parsed_content)
    chunked_content = chunk_content(combined_content, max_chunk_size=500)

    # Choose embedding model (optional domain specification)
    model_name = choose_embedding_model()  # Or specify a domain

    # Initialize retriever
    retriever = initialize_faiss_index_and_chain(
        chunked_content,
        model_name=model_name
    )

    if retriever:
        # Example queries
        queries = [
            "Summarize the introduction slide",
            "What are the key points about the topic?",
            "Provide a detailed overview"

        ]

        for query in queries:
            print(f"\nQuery: {query}")
            response = query_with_groq(retriever, query)
            print("Response:", response)

# Run the main function
if __name__ == "__main__":
    main()





# from groq import Groq
# import os
# from langchain.embeddings import SentenceTransformerEmbeddings
# from langchain.vectorstores import FAISS
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from sentence_transformers import SentenceTransformer

# # Initialize Groq client with the API key directly (set the actual key here or from environment)
# api_key = os.getenv("GROQ_API_KEY", "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")  # Using environment variable
# client = Groq(api_key=api_key)  # Initialize client once

# # Function to initialize FAISS index and load embeddings into LangChain
# def initialize_faiss_index_and_chain(chunked_content):
#     """
#     Initialize FAISS index with LangChain and return a retrieval mechanism.
#     """
#     # Generate embeddings using LangChain
#     embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
#     embedding_function = SentenceTransformerEmbeddings(embedding_model)

#     # Create FAISS vector store from chunked content and embeddings
#     vector_store = FAISS.from_texts(chunked_content, embedding_function)

#     # Create a retriever from the vector store
#     retriever = vector_store.as_retriever()

#     return retriever  # Only return retriever for use with Groq LLM

# # Function to query the Groq LLM with the retriever
# def query_with_groq(retriever, query, model="llama3-8b-8192", temperature=1, max_tokens=1024):
#     """
#     Use Groq's LLM to answer the query based on retrieved documents.
#     """
#     # Retrieve relevant documents
#     relevant_docs = retriever.get_relevant_documents(query)
#     context = "\n\n".join([doc.page_content for doc in relevant_docs])

#     # Formulate messages for the chat completion
#     messages = [
#         {"role": "system", "content": "You are an assistant that answers questions based on the provided context."},
#         {"role": "user", "content": f"Context:\n{context}\n\nQuery: {query}"}
#     ]

#     # Call the Groq client
#     completion = client.chat.completions.create(
#         model=model,
#         messages=messages,
#         temperature=temperature,
#         max_tokens=max_tokens,
#         top_p=1,
#         stream=True,
#         stop=None,
#     )

#     # Collect the response
#     response = ""
#     for chunk in completion:
#         response += chunk.choices[0].delta.content or ""

#     return response

# # Consolidate content into chunks for processing (Ensure these functions are defined properly)
# combined_content = consolidate_content(parsed_content)
# chunked_content = chunk_content(combined_content, max_chunk_size=500)

# # Initialize FAISS index and retriever
# retriever = initialize_faiss_index_and_chain(chunked_content)

# # Test the system with a sample query using Groq LLM
# query = "Summarize the introduction slide and extract key points."
# response = query_with_groq(retriever, query)
# print("Response:", response)

# import os

# # Set your API key
# os.environ["OPENAI_API_KEY"] = "your_openai_api_key_here"  # Replace with your actual key

# # Initialize the LLM with the API key
# def initialize_faiss_index_and_chain(chunked_content):
#     """
#     Initialize FAISS index with LangChain and return a QA retrieval chain.
#     """
#     # Generate embeddings using LangChain
#     embedding_model = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
#     vector_store = FAISS.from_texts(chunked_content, embedding_model)

#     # Define the LLM to use (OpenAI or GROQ cloud)
#     llm = ChatOpenAI(
#         openai_api_key=os.getenv("OPENAI_API_KEY"),  # Use the key from environment
#         model="gpt-4",
#         temperature=0.7
#     )

#     # Create a retrieval-based QA chain
#     qa_chain = RetrievalQA.from_chain_type(
#         llm=llm,
#         chain_type="stuff",
#         retriever=vector_store.as_retriever()
#     )
#     return qa_chain

# def query_chain(qa_chain, query):
#     """
#     Query the QA chain and return the answer.
#     """
#     return qa_chain.run(query)

# # Consolidate content into chunks for LangChain processing
# combined_content = consolidate_content(parsed_content)
# chunked_content = chunk_content(combined_content, max_chunk_size=500)

# # Initialize FAISS index and retrieval-based QA chain
# qa_chain = initialize_faiss_index_and_chain(chunked_content)

# # Test the system with a sample query
# query = "Summarize the introduction slide and extract key points."
# response = query_chain(qa_chain, query)
# print("Response:", response)

# from groq import Groq
# import os
# #from langchain.embeddings import SentenceTransformer
# from langchain import FAISS
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.embeddings import OpenAIEmbeddings  # Import OpenAIEmbeddings

# # Initialize Groq client with the API key directly (set the actual key here or from environment)
# api_key = os.getenv("GROQ_API_KEY", "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")  # Using environment variable
# client = Groq(api_key=api_key)  # Initialize client once

# # Function to initialize FAISS index and load embeddings into LangChain
# def initialize_faiss_index_and_chain(chunked_content):
#     """
#     Initialize FAISS index with LangChain and return a retrieval mechanism.
#     """
#     # Generate embeddings using LangChain
#     embedding_model = OpenAIEmbeddings()
#     vector_store = FAISS.from_texts(chunked_content, embedding_model)

#     # Create a retriever from the vector store
#     retriever = vector_store.as_retriever()

#     return retriever  # Only return retriever for use with Groq LLM

# # Function to query the Groq LLM with the retriever
# def query_with_groq(retriever, query, model="llama3-8b-8192", temperature=1, max_tokens=1024):
#     """
#     Use Groq's LLM to answer the query based on retrieved documents.
#     """
#     # Retrieve relevant documents
#     relevant_docs = retriever.get_relevant_documents(query)
#     context = "\n\n".join([doc.page_content for doc in relevant_docs])

#     # Formulate messages for the chat completion
#     messages = [
#         {"role": "system", "content": "You are an assistant that answers questions based on the provided context."},
#         {"role": "user", "content": f"Context:\n{context}\n\nQuery: {query}"}
#     ]

#     # Call the Groq client
#     completion = client.chat.completions.create(
#         model=model,
#         messages=messages,
#         temperature=temperature,
#         max_tokens=max_tokens,
#         top_p=1,
#         stream=True,
#         stop=None,
#     )

#     # Collect the response
#     response = ""
#     for chunk in completion:
#         response += chunk.choices[0].delta.content or ""

#     return response

# # Consolidate content into chunks for processing (Ensure these functions are defined properly)
# combined_content = consolidate_content(parsed_content)
# chunked_content = chunk_content(combined_content, max_chunk_size=500)

# # Initialize FAISS index and retriever
# retriever = initialize_faiss_index_and_chain(chunked_content)

# # Test the system with a sample query using Groq LLM
# query = "Summarize the introduction slide and extract key points."
# response = query_with_groq(retriever, query)
# print("Response:", response)